<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hcimaker.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hcimaker.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-03T21:19:18+00:00</updated><id>https://hcimaker.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Attention Is All You Need Explained By Non-CS Major</title><link href="https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need/" rel="alternate" type="text/html" title="Attention Is All You Need Explained By Non-CS Major"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need</id><content type="html" xml:base="https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need/"><![CDATA[<p><a href="https://arxiv.org/pdf/1706.03762">Original Paper</a></p> <h2 id="modified-heilmeier-catechism">Modified Heilmeier Catechism</h2> <ol> <li> <p>What does the paper try to do?<br/> The paper proposes a novel deep learning model structure for sequence-to-sequence tasks, largely improving the computation efficieny (reduction of training time) of the deep learning model. The paper also inspires the ubiquitious usage of the novel model structure to the generation task.</p> </li> <li> <p>What are the limits of the practice at that time?<br/> The computation times of relating two arbitrary input or output positions grows in the distance between positions.(linearly or logarithmically)</p> </li> <li> <p>What is novel in this paper? And why it is successful?<br/> The novel deep learning model structure are fully based on the attention mechanism relating tokens at different positions of a single(multiple) sequence(s). This attention mechanism allows a more significant parallelization during the training stage, and it achieves a better performance at the same time.</p> </li> <li> <p>What is the impact?<br/> The attention structure becomes the foundation of the current revolutionized AI-Generated Content (AIGC) technology trend due to its striking improvement on the computation efficacy.</p> </li> </ol> <h2 id="model-structure">Model Structure</h2> <p><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Source Website: The Annotated Transformer</a> to-be-continued</p>]]></content><author><name></name></author><category term="IIya"/><category term="30u30"/><category term="Machine-Learning"/><summary type="html"><![CDATA[Original Paper]]></summary></entry><entry><title type="html">Tomography Through IR Light</title><link href="https://hcimaker.github.io/blog/2024/TomoIR/" rel="alternate" type="text/html" title="Tomography Through IR Light"/><published>2024-10-11T00:00:00+00:00</published><updated>2024-10-11T00:00:00+00:00</updated><id>https://hcimaker.github.io/blog/2024/TomoIR</id><content type="html" xml:base="https://hcimaker.github.io/blog/2024/TomoIR/"><![CDATA[<p>Hand gesture identification is still one of the most compelling and difficult missions in the world of human-computer interaction, especially in AR/VR technique. To tackle this challenge, many approaches have been explored such as computer-vision-based sensing, electromyography, and ultrasound sensing. In this project, we propose a novel sensing method based on the Infrared (IR) Light. The IR light at 850nm has a strong penetrability through human’s body. If we pose IR light on the wrist, the interior muscle structure change will be shown by IR light.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/88-480.webp 480w,/assets/img/88-800.webp 800w,/assets/img/88-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/88.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/90-480.webp 480w,/assets/img/90-800.webp 800w,/assets/img/90-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/90.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The left-handside figure shows the IR camera view, proving we can see the IR light from our wrist. The right-hand side shows our quick 3D-prototype wristband. </div> <p>Currently, we are processing the raw data from photodiodes and the number of IR LED and photodiode on the wristband is quite small. This is the reason why we can only identify a small number of hand gestures with big changes. As mentioned in the title, we will implement the tomography as the feature of the machine learning pipeline and increase the number of pair of IR LED and photodiode in our next step. Check out this quick demo.</p> <p><iframe src="https://www.youtube.com/embed/_R0YtRDgjHU" frameborder="0" allowfullscreen=""></iframe></p> <blockquote> <p>Life would be too smooth if it had no rubs in it.</p> </blockquote>]]></content><author><name></name></author><category term="Embedded-System"/><summary type="html"><![CDATA[Hand gesture identification is still one of the most compelling and difficult missions in the world of human-computer interaction, especially in AR/VR technique. To tackle this challenge, many approaches have been explored such as computer-vision-based sensing, electromyography, and ultrasound sensing. In this project, we propose a novel sensing method based on the Infrared (IR) Light. The IR light at 850nm has a strong penetrability through human’s body. If we pose IR light on the wrist, the interior muscle structure change will be shown by IR light.]]></summary></entry></feed>