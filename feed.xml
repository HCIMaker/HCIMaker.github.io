<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://hcimaker.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hcimaker.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-22T16:07:35+00:00</updated><id>https://hcimaker.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Attention Is All You Need Explained By Non-CS Major</title><link href="https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need/" rel="alternate" type="text/html" title="Attention Is All You Need Explained By Non-CS Major"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need</id><content type="html" xml:base="https://hcimaker.github.io/blog/2024/Attention-Is-Your-Need/"><![CDATA[<p><a href="https://arxiv.org/pdf/1706.03762">Original Paper</a></p> <h2 id="modified-heilmeier-catechism">Modified Heilmeier Catechism</h2> <ol> <li> <p>What does the paper try to do?<br/> The paper proposes a novel deep learning model structure for sequence-to-sequence tasks, largely improving the computation efficieny (reduction of training time) of the deep learning model. The paper also inspires the ubiquitious usage of the novel model structure to the generation task.</p> </li> <li> <p>What are the limits of the practice at that time?<br/> The computation times of relating two arbitrary input or output positions grows in the distance between positions.(linearly or logarithmically)</p> </li> <li> <p>What is novel in this paper? And why it is successful?<br/> The novel deep learning model structure are fully based on the attention mechanism relating tokens at different positions of a single(multiple) sequence(s). This attention mechanism allows a more significant parallelization during the training stage, and it achieves a better performance at the same time.</p> </li> <li> <p>What is the impact?<br/> The attention structure becomes the foundation of the current revolutionized AI-Generated Content (AIGC) technology trend due to its striking improvement on the computation efficacy.</p> </li> </ol> <h2 id="model-structure">Model Structure</h2> <p><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Source Website: The Annotated Transformer</a> to-be-continued</p>]]></content><author><name></name></author><category term="IIya"/><category term="30u30"/><category term="Machine-Learning"/><summary type="html"><![CDATA[Original Paper]]></summary></entry></feed>